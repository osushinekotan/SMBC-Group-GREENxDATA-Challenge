{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.experiment.optimization import find_optimal_threshold_for_label, decode_label\n",
    "from src.experiment.model.custom_metrics import lgb_macro_auc, lgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=056-stacking_v2\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)\n",
    "BASE_OUTPUT_DIR = Path(CFG.paths.resource_dir) / \"outputs\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_multiclass_ensemble_feature_df(ensemble_exps: list[str]):\n",
    "    feature_df = pd.DataFrame()\n",
    "    for exp in ensemble_exps:\n",
    "        logger.info(f\"Load {exp} ...\")\n",
    "        train_filepath = BASE_OUTPUT_DIR / exp / \"valid_result_df.pkl\"\n",
    "        test_filepath = BASE_OUTPUT_DIR / exp / \"test_result_df.pkl\"\n",
    "\n",
    "        train_result_df = joblib.load(train_filepath).set_index(\"uid\")\n",
    "        test_result_df = joblib.load(test_filepath)\n",
    "\n",
    "        test_pred_df = test_result_df[[\"uid\", 0, 1, 2]].groupby(\"uid\").mean().add_prefix(f\"{exp}_\")\n",
    "        train_pred_df = train_result_df[\"pred\"].apply(pd.Series).add_prefix(f\"{exp}_\")\n",
    "        df = pd.concat([train_pred_df, test_pred_df], axis=0, ignore_index=False).add_prefix(\"f_\")\n",
    "        feature_df = pd.concat([feature_df, df], axis=1)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def make_pred_label_df(ensemble_exps: list[str]):\n",
    "    feature_df = pd.DataFrame()\n",
    "    for exp in ensemble_exps:\n",
    "        logger.info(f\"Load {exp} ...\")\n",
    "        train_filepath = BASE_OUTPUT_DIR / exp / \"valid_result_df.pkl\"\n",
    "        test_filepath = BASE_OUTPUT_DIR / exp / \"test_result_df.pkl\"\n",
    "\n",
    "        train_result_df = joblib.load(train_filepath)\n",
    "        test_result_df = joblib.load(test_filepath)\n",
    "\n",
    "        test_pred_df_ = test_result_df[[\"uid\", 0, 1, 2]].groupby(\"uid\").mean()\n",
    "        test_pred_df_[\"pred\"] = test_pred_df_.to_numpy().tolist()\n",
    "        test_pred_df = test_pred_df_[[\"pred\"]].reset_index()\n",
    "        opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "            proba_matrix=np.array(train_result_df[\"pred\"].to_numpy().tolist()),\n",
    "            true_labels=train_result_df[\"health\"],\n",
    "            label_indices=[2, 0, 1],\n",
    "        )\n",
    "\n",
    "        best_f1_score = f1_score(\n",
    "            y_true=train_result_df[\"health\"],\n",
    "            y_pred=val_pred_label,\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "        logger.info(f\"best f1 score: {best_f1_score:.4f}\")\n",
    "\n",
    "        train_pred_df = (\n",
    "            train_result_df[[\"uid\"]]\n",
    "            .assign(pred_label=val_pred_label)\n",
    "            .set_index(\"uid\")\n",
    "            .add_prefix(f\"{exp}_\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        test_pred_df[\"pred_label\"] = decode_label(\n",
    "            proba_matrix=np.array(test_pred_df[\"pred\"].to_numpy().tolist()), thresholds=opt_results\n",
    "        )\n",
    "        test_pred_df = test_pred_df[[\"uid\", \"pred_label\"]].set_index(\"uid\").add_prefix(f\"{exp}_\").reset_index()\n",
    "\n",
    "        df = pd.concat(\n",
    "            [train_pred_df.set_index(\"uid\"), test_pred_df.set_index(\"uid\")],\n",
    "            axis=0,\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        feature_df = pd.concat([feature_df, df], axis=1)\n",
    "\n",
    "    return feature_df.add_prefix(\"f_\")\n",
    "\n",
    "\n",
    "def voting(candidates: list[int], priorities: list[int]):\n",
    "    vote_counts = {candidate: candidates.count(candidate) for candidate in set(candidates)}\n",
    "    max_votes = max(vote_counts.values())\n",
    "    potential_winners = [candidate for candidate, votes in vote_counts.items() if votes == max_votes]\n",
    "\n",
    "    if len(potential_winners) == 1:\n",
    "        return potential_winners[0]\n",
    "    for priority in priorities:\n",
    "        if priority in potential_winners:\n",
    "            return priority\n",
    "\n",
    "\n",
    "ensemble_pred_label_df = make_pred_label_df(ensemble_exps=CFG.ensemble_exps)\n",
    "ensemble_pred_label_df[\"f_pred_label\"] = ensemble_pred_label_df.apply(\n",
    "    lambda row: voting(row.tolist(), priorities=[2, 0, 1]), axis=1\n",
    ").tolist()\n",
    "\n",
    "\n",
    "ensemble_pred_df = make_multiclass_ensemble_feature_df(ensemble_exps=CFG.ensemble_exps)\n",
    "raw_df = pd.merge(raw_df, ensemble_pred_df, on=\"uid\", how=\"left\")\n",
    "raw_df = pd.merge(raw_df, ensemble_pred_label_df, on=\"uid\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_agg_feature_extractors(feature_extractors, all_group_keys):\n",
    "    if feature_extractors is None:\n",
    "        return []\n",
    "\n",
    "    if all_group_keys is None:\n",
    "        return []\n",
    "\n",
    "    extractors = []\n",
    "    for extractor in feature_extractors:\n",
    "        for group_keys in all_group_keys:\n",
    "            _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "            extractors.append(_extractor)\n",
    "    return extractors\n",
    "\n",
    "\n",
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "feature_extractors.extend(get_agg_feature_extractors(CFG.get(\"agg_feature_extractors\"), CFG.get(\"group_keys_for_agg\")))\n",
    "feature_extractors.extend(get_agg_feature_extractors(CFG.get(\"te_feature_extractors\"), CFG.get(\"group_keys_for_te\")))\n",
    "feature_extractors.extend(\n",
    "    get_agg_feature_extractors(CFG.get(\"rolling_agg_feature_extractors\"), CFG.get(\"group_keys_for_rolling_agg\"))\n",
    ")\n",
    "\n",
    "raw_feature_df = run_extractors(\n",
    "    input_df=raw_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "assert len(raw_df) == len(raw_feature_df)\n",
    "\n",
    "raw_feature_df = pd.concat([raw_df, raw_feature_df], axis=1)\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True).astype({\"health\": int})\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]\n",
    "logger.info(f\"num features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_seed_average_pred(result_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [col for col in result_df.columns if col.startswith(\"pred_\")]\n",
    "    pred = 0\n",
    "    for col in cols:\n",
    "        pred += np.array(result_df[col].tolist())\n",
    "    result_df[\"pred\"] = list(pred / len(cols))\n",
    "    return result_df.reset_index()\n",
    "\n",
    "\n",
    "valid_result_df = pd.DataFrame()\n",
    "all_trained_estimators = []\n",
    "scores = {}\n",
    "for seed in CFG.seed_average_seeds:\n",
    "    logger.info(f\"\\n\\nstart training seed={seed} ðŸš€\")\n",
    "    CFG.model.estimator.random_state = seed\n",
    "\n",
    "    fit_params = dict(hydra.utils.instantiate(CFG.model.fit_params))\n",
    "    if CFG.model.estimator._target_.startswith(\"lightgbm.LGBM\"):\n",
    "        fit_params[\"eval_metric\"] = [\n",
    "            # lgb_macro_auc,\n",
    "            lgb_macro_f1,\n",
    "        ]\n",
    "        CFG.model.estimator.num_leaves = seed  # lgbm\n",
    "\n",
    "    if CFG.use_cat_features:\n",
    "        cat_features = [x for x in feature_columns if x.startswith(\"f_oe_\")]\n",
    "        estimator = hydra.utils.instantiate(CFG.model.estimator, cat_features=cat_features)\n",
    "    else:\n",
    "        estimator = hydra.utils.instantiate(CFG.model.estimator)\n",
    "\n",
    "    model_output_dir = OUTPUT_DIR / \"models\" / f\"seed{seed}\"\n",
    "    trained_estimators = train_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimator=estimator,\n",
    "        feature_columns=feature_columns,\n",
    "        target_columns=[\"health\"],\n",
    "        fit_params=fit_params,\n",
    "        output_dir=model_output_dir,\n",
    "        overwrite=CFG.overwrite_training,\n",
    "        use_eval_set=CFG.model.use_eval_set,\n",
    "    )\n",
    "\n",
    "    i_valid_result_df = predict_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimators=trained_estimators,\n",
    "        feature_columns=feature_columns,\n",
    "        predict_proba=CFG.model.predict_proba,\n",
    "    )\n",
    "    val_score = macro_f1_from_proba(\n",
    "        y_true=i_valid_result_df[\"health\"],\n",
    "        y_pred=i_valid_result_df[\"pred\"].tolist(),\n",
    "    )\n",
    "    logger.info(f\"macro f1 score [seed={seed}]: {val_score}\")\n",
    "    scores[f\"seed{seed}\"] = val_score\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [\n",
    "            valid_result_df,\n",
    "            i_valid_result_df[[\"uid\", \"pred\", \"health\"]]\n",
    "            .set_index([\"uid\", \"health\"])\n",
    "            .rename(columns={\"pred\": f\"pred_{seed}\"}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    all_trained_estimators.extend(trained_estimators)\n",
    "\n",
    "\n",
    "valid_result_df = assign_seed_average_pred(valid_result_df)\n",
    "val_proba = np.array(valid_result_df[\"pred\"].tolist())\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=val_proba)\n",
    "scores[\"all_nomal\"] = val_score\n",
    "\n",
    "opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "    proba_matrix=val_proba,\n",
    "    true_labels=valid_result_df[\"health\"],\n",
    "    label_indices=[2, 0, 1],\n",
    ")\n",
    "best_f1_score = f1_score(\n",
    "    y_true=valid_result_df[\"health\"],\n",
    "    y_pred=val_pred_label,\n",
    "    average=\"macro\",\n",
    ")\n",
    "scores[\"all_opt\"] = best_f1_score\n",
    "\n",
    "logger.info(f\"macro f1 score: {val_score}\")\n",
    "logger.info(f\"optimized thresholds: {opt_results}\")\n",
    "logger.info(f\"best f1 score: {best_f1_score}\")\n",
    "\n",
    "joblib.dump(valid_result_df[[\"uid\", \"health\", \"pred\"]], OUTPUT_DIR / \"valid_result_df.pkl\")\n",
    "json.dump(scores, open(OUTPUT_DIR / \"scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")\n",
    "fig.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "importance_df.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n",
    "\n",
    "\n",
    "fig = plot_label_distributions(proba_matrix=np.array(valid_result_df[\"pred\"].tolist()))\n",
    "fig.show()\n",
    "fig.savefig(OUTPUT_DIR / \"label_distributions.png\", dpi=300)\n",
    "\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label, normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    "    predict_proba=CFG.model.predict_proba,\n",
    ")\n",
    "\n",
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"pred\"] = np.argmax(test_pred_df.groupby(\"uid\").mean(), axis=1)\n",
    "submission_df = test_df[[\"uid\", \"pred\"]]\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"opt_pred\"] = decode_label(proba_matrix=test_pred_df.groupby(\"uid\").mean().to_numpy(), thresholds=opt_results)\n",
    "\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_opt_{best_f1_score:.3f}.csv\"\n",
    "test_df[[\"uid\", \"opt_pred\"]].to_csv(submission_filepath, index=False, header=False)\n",
    "\n",
    "joblib.dump(test_pred_df, OUTPUT_DIR / \"test_result_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df[\"opt_pred\"].value_counts() / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"pred\"].value_counts() / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"health\"].value_counts() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_by_color(df, value_col, color_col):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for color in df[color_col].unique():\n",
    "        sns.distplot(df[value_col][df[color_col] == color], hist=False, label=color)\n",
    "\n",
    "    plt.title(\"Distribution of Values Color-Coded by Color\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
