{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import re\n",
    "import json\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "from datasets import Dataset, DatasetDict\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    EvalPrediction,\n",
    "    Trainer,\n",
    ")\n",
    "from torch import nn\n",
    "import torch\n",
    "from transformers import Trainer\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.experiment.optimization import find_optimal_threshold_for_label, decode_label\n",
    "from src.experiment.model.custom_metrics import lgb_macro_auc, lgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=067-text_v1\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "def align_train_test_unique(train, test, ignore_columns=[\"uid\", \"data\", \"fold\", \"created_at\"], fill_value=np.nan):\n",
    "    \"\"\"\n",
    "    This function modifies both 'train' and 'test' DataFrames. For each column,\n",
    "    it replaces values that are unique to either set (not common to both) with NaN.\n",
    "\n",
    "    :param train: DataFrame used for training.\n",
    "    :param test: DataFrame used for testing.\n",
    "    :return: Tuple of modified train and test DataFrames.\n",
    "    \"\"\"\n",
    "    aligned_train = train.copy()\n",
    "    aligned_test = test.copy()\n",
    "\n",
    "    for column in train.columns:\n",
    "        if column in ignore_columns:\n",
    "            continue\n",
    "        if column in test.columns:\n",
    "            # Find values that are not common to both train and test sets\n",
    "            common_values = set(train[column]).intersection(set(test[column]))\n",
    "\n",
    "            aligned_train[column] = train[column].apply(lambda x: x if x in common_values else fill_value)\n",
    "            aligned_test[column] = test[column].apply(lambda x: x if x in common_values else fill_value)\n",
    "\n",
    "    return aligned_train, aligned_test\n",
    "\n",
    "\n",
    "def replace_rare_values(df, cols, threshold, replacement_value):\n",
    "    \"\"\"\n",
    "    This function replaces values in each column of the DataFrame that appear less frequently\n",
    "    than the specified threshold with a specified replacement value.\n",
    "\n",
    "    :param df: DataFrame to process.\n",
    "    :param threshold: Frequency threshold. Values appearing less than this will be replaced.\n",
    "    :param replacement_value: The value to replace rare values with.\n",
    "    :return: Modified DataFrame.\n",
    "    \"\"\"\n",
    "    for column in cols:\n",
    "        value_counts = df[column].value_counts()\n",
    "        values_to_replace = value_counts[value_counts < threshold].index\n",
    "\n",
    "        df[column] = df[column].apply(lambda x: replacement_value if x in values_to_replace else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "if CFG.debug:\n",
    "    train_df = train_df.sample(10).reset_index(drop=True)\n",
    "    test_df = test_df.sample(10).reset_index(drop=True)\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")\n",
    "\n",
    "if CFG.align_train_test:\n",
    "    train_df, test_df = align_train_test_unique(\n",
    "        train=train_df,\n",
    "        test=test_df,\n",
    "        ignore_columns=[\n",
    "            \"uid\",\n",
    "            \"data\",\n",
    "            \"fold\",\n",
    "            \"created_at\",\n",
    "            \"tree_dbh\",\n",
    "            # \"spc_common\",\n",
    "            # \"spc_latin\",\n",
    "        ],\n",
    "    )  #  test ã«ãªã„ã‚‚ã®ã¯ nan ã«ã™ã‚‹\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype == \"float\":\n",
    "        continue\n",
    "    logger.info(f\"{col}: {train_df[col].nunique()}, {test_df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "if CFG.replace_rare_values_threshold is not None:\n",
    "    raw_df = replace_rare_values(\n",
    "        df=raw_df,\n",
    "        cols=[\n",
    "            \"spc_common\",\n",
    "            \"spc_latin\",\n",
    "            \"boro_ct\",\n",
    "            \"cb_num\",\n",
    "            \"st_assem\",\n",
    "            \"nta\",\n",
    "            \"nta_name\",\n",
    "            \"zip_city\",\n",
    "            \"borocode\",\n",
    "            \"boroname\",\n",
    "            \"st_senate\",\n",
    "            \"cncldist\",\n",
    "        ],\n",
    "        threshold=CFG.replace_rare_values_threshold,\n",
    "        replacement_value=np.nan,\n",
    "    )\n",
    "\n",
    "    for col in raw_df.columns:\n",
    "        logger.info(f\"{col}: {raw_df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_text(row):\n",
    "    def _parse(text):\n",
    "        if text == \"None\":\n",
    "            return text\n",
    "        elements = re.split(\"(?=[A-Z])\", text)[1:]\n",
    "        return \" \".join(elements).lower()\n",
    "\n",
    "    def _parse_steward(steward):\n",
    "        if steward == \"None\":\n",
    "            return \"no steward\"\n",
    "        elif steward == \"1or2\":\n",
    "            return \"1 or 2\"\n",
    "        elif steward == \"3or4\":\n",
    "            return \"3 or 4\"\n",
    "        elif steward == \"4orMore\":\n",
    "            return \"4 or more\"\n",
    "        else:\n",
    "            return steward\n",
    "\n",
    "    # created_at = f'created at {row[\"created_at\"]}'\n",
    "    # tree_dbh = f'tree diameter {row[\"tree_dbh\"]}'\n",
    "    # curb_loc = f'curb location {_parse(row[\"curb_loc\"])}'\n",
    "    # steward = f'steward {_parse_steward(row[\"steward\"])}'\n",
    "    # guards = f'guards {(row[\"guards\"])}'\n",
    "    # sidewalk = f'sidewalk condition {_parse(row[\"sidewalk\"])}'\n",
    "    # user_type = f'user type {row[\"user_type\"]}'\n",
    "    # problems = f'problems {_parse(row[\"problems\"])}'\n",
    "    # spc_common = f'species common name {row[\"spc_common\"]}'\n",
    "    # spc_latin = f'species latin name {row[\"spc_latin\"]}'\n",
    "    # nta_name = f'neighborhood tabulation area name {row[\"nta_name\"]}'\n",
    "    # borocode = f'borough code {row[\"borocode\"]}'\n",
    "    # boroname = f'borough name {row[\"boroname\"]}'\n",
    "    # zip_city = f'zip city {row[\"zip_city\"]}'\n",
    "    # boro_ct = f'boro ct {row[\"boro_ct\"]}'\n",
    "    # cb_num = f'cb num {row[\"cb_num\"]}'\n",
    "    # st_senate = f'st senate {row[\"st_senate\"]}'\n",
    "    # st_assem = f'st assem {row[\"st_assem\"]}'\n",
    "    # cncldist = f'cncldist {row[\"cncldist\"]}'\n",
    "    created_at = f'created at {row[\"created_at\"]}'\n",
    "    tree_dbh = f'tree diameter {row[\"tree_dbh\"]}'\n",
    "    curb_loc = f'{_parse(row[\"curb_loc\"])}'\n",
    "    steward = f'{_parse_steward(row[\"steward\"])}'\n",
    "    guards = f'{(row[\"guards\"])}'\n",
    "    sidewalk = f'{_parse(row[\"sidewalk\"])}'\n",
    "    user_type = f'{row[\"user_type\"]}'\n",
    "    problems = f'{_parse(row[\"problems\"])}'\n",
    "    spc_common = f'{row[\"spc_common\"]}'\n",
    "    spc_latin = f'{row[\"spc_latin\"]}'\n",
    "    nta_name = f'{row[\"nta_name\"]}'\n",
    "    borocode = f'{row[\"borocode\"]}'\n",
    "    boroname = f'{row[\"boroname\"]}'\n",
    "    zip_city = f'{row[\"zip_city\"]}'\n",
    "    boro_ct = f'{row[\"boro_ct\"]}'\n",
    "    cb_num = f'{row[\"cb_num\"]}'\n",
    "    st_senate = f'{row[\"st_senate\"]}'\n",
    "    st_assem = f'{row[\"st_assem\"]}'\n",
    "    cncldist = f'{row[\"cncldist\"]}'\n",
    "\n",
    "    text = \" \".join(\n",
    "        [\n",
    "            created_at,\n",
    "            tree_dbh,\n",
    "            curb_loc,\n",
    "            steward,\n",
    "            guards,\n",
    "            sidewalk,\n",
    "            user_type,\n",
    "            problems,\n",
    "            spc_common,\n",
    "            spc_latin,\n",
    "            nta_name,\n",
    "            borocode,\n",
    "            boroname,\n",
    "            zip_city,\n",
    "            boro_ct,\n",
    "            cb_num,\n",
    "            st_senate,\n",
    "            st_assem,\n",
    "            cncldist,\n",
    "        ]\n",
    "    )\n",
    "    return text\n",
    "\n",
    "\n",
    "def get_datasets(df: pd.DataFrame, tokenizer, max_len: int):\n",
    "    def text_to_input_ids(examples):\n",
    "        return tokenizer(examples[\"text\"], padding=False, truncation=True, max_length=max_len)\n",
    "\n",
    "    # convert [SEP] to tokenizer.sep_token\n",
    "    df[\"text\"] = df[\"text\"].replace(\"[SEP]\", tokenizer.sep_token)\n",
    "\n",
    "    ds = Dataset.from_pandas(df)\n",
    "    return ds.map(text_to_input_ids, batched=True, num_proc=max([1, cpu_count() - 1]))\n",
    "\n",
    "\n",
    "def compute_metrics(p: EvalPrediction):\n",
    "    preds = p.predictions\n",
    "    labels = p.label_ids\n",
    "    pred_labels = np.argmax(preds, axis=1)\n",
    "    score = f1_score(labels, pred_labels, average=\"macro\")\n",
    "    metrics = {\"f1_score\": score}\n",
    "    return metrics\n",
    "\n",
    "\n",
    "train_df[\"text\"] = train_df.fillna(\"None\").apply(make_text, axis=1)\n",
    "test_df[\"text\"] = test_df.fillna(\"None\").apply(make_text, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train & Inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        # forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get(\"logits\")\n",
    "        # compute custom loss (suppose one has 3 labels with different weights)\n",
    "        loss_fct = nn.CrossEntropyLoss(\n",
    "            weight=torch.tensor(\n",
    "                [\n",
    "                    1.88439415,\n",
    "                    0.42291495,\n",
    "                    9.5434575,\n",
    "                ],\n",
    "                device=model.device,\n",
    "            ).float()\n",
    "        )\n",
    "        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_result_df = pd.DataFrame()\n",
    "test_result_df = pd.DataFrame()\n",
    "scores = {}\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(CFG.transformer_model)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "for i_fold in np.sort(train_df[\"fold\"].unique()):\n",
    "    logger.info(f\"fold{i_fold} start!!! ðŸš€\")\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(CFG.transformer_model, num_labels=3)\n",
    "    i_train_df = train_df[train_df[\"fold\"] != i_fold]\n",
    "    i_valid_df = train_df[train_df[\"fold\"] == i_fold]\n",
    "\n",
    "    train_ds = get_datasets(\n",
    "        df=i_train_df[[\"uid\", \"health\", \"text\"]].rename(columns={\"health\": \"labels\"}),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=CFG.max_length,\n",
    "    )\n",
    "    valid_ds = get_datasets(\n",
    "        df=i_valid_df[[\"uid\", \"health\", \"text\"]].rename(columns={\"health\": \"labels\"}),\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=CFG.max_length,\n",
    "    )\n",
    "    test_ds = get_datasets(\n",
    "        df=test_df[[\"uid\", \"text\"]],\n",
    "        tokenizer=tokenizer,\n",
    "        max_len=CFG.max_length,\n",
    "    )\n",
    "\n",
    "    train_args = hydra.utils.instantiate(CFG.train_args, output_dir=OUTPUT_DIR / f\"fold{i_fold}\")\n",
    "    trainer = CustomTrainer(\n",
    "        model=model,\n",
    "        args=train_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=valid_ds,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # predict on valid and test by trained model\n",
    "    val_pred_result = trainer.predict(valid_ds)\n",
    "\n",
    "    # calc val score\n",
    "    tmp_val_pred_label = np.argmax(val_pred_result.predictions, axis=1)\n",
    "    val_score = f1_score(val_pred_result.label_ids, tmp_val_pred_label, average=\"macro\")\n",
    "    logger.info(f\"fold{i_fold} val score: {val_score}\")\n",
    "\n",
    "    # store valid and test prediction\n",
    "    i_valid_result_df = i_valid_df.assign(pred=val_pred_result.predictions.tolist())\n",
    "    valid_result_df = pd.concat([valid_result_df, i_valid_result_df], axis=0)\n",
    "\n",
    "    test_pred = trainer.predict(test_ds).predictions\n",
    "    i_test_result_df = test_df.assign(pred=test_pred.tolist())\n",
    "    test_result_df = pd.concat([test_result_df, i_test_result_df], axis=0)\n",
    "\n",
    "# reindex\n",
    "valid_result_df = valid_result_df.sort_values(\"uid\").reset_index(drop=True)\n",
    "test_result_df = test_result_df.sort_values(\"uid\").reset_index(drop=True)\n",
    "\n",
    "val_proba = np.array(valid_result_df[\"pred\"].tolist())\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=val_proba)\n",
    "scores[\"all_nomal\"] = val_score\n",
    "\n",
    "opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "    proba_matrix=val_proba,\n",
    "    true_labels=valid_result_df[\"health\"],\n",
    "    label_indices=[2, 0, 1],\n",
    ")\n",
    "best_f1_score = f1_score(\n",
    "    y_true=valid_result_df[\"health\"],\n",
    "    y_pred=val_pred_label,\n",
    "    average=\"macro\",\n",
    ")\n",
    "scores[\"all_opt\"] = best_f1_score\n",
    "\n",
    "logger.info(f\"macro f1 score: {val_score}\")\n",
    "logger.info(f\"optimized thresholds: {opt_results}\")\n",
    "logger.info(f\"best f1 score: {best_f1_score}\")\n",
    "\n",
    "joblib.dump(valid_result_df[[\"uid\", \"health\", \"pred\"]], OUTPUT_DIR / \"valid_result_df.pkl\")\n",
    "json.dump(scores, open(OUTPUT_DIR / \"scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_label_distributions(proba_matrix=np.array(valid_result_df[\"pred\"].tolist()))\n",
    "fig.show()\n",
    "fig.savefig(OUTPUT_DIR / \"label_distributions.png\", dpi=300)\n",
    "\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label, normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"pred\"] = np.argmax(test_pred_df.groupby(\"uid\").mean(), axis=1)\n",
    "submission_df = test_df[[\"uid\", \"pred\"]]\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"opt_pred\"] = decode_label(proba_matrix=test_pred_df.groupby(\"uid\").mean().to_numpy(), thresholds=opt_results)\n",
    "\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_opt_{best_f1_score:.3f}.csv\"\n",
    "test_df[[\"uid\", \"opt_pred\"]].to_csv(submission_filepath, index=False, header=False)\n",
    "\n",
    "joblib.dump(test_pred_df, OUTPUT_DIR / \"test_result_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
