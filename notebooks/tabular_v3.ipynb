{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.experiment.optimization import find_optimal_threshold_for_label, decode_label\n",
    "from src.experiment.model.custom_metrics import lgb_macro_auc, lgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=047-tabular_v3\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "def align_train_test_unique(train, test, ignore_columns=[\"uid\", \"data\", \"fold\", \"created_at\"], fill_value=np.nan):\n",
    "    \"\"\"\n",
    "    This function modifies both 'train' and 'test' DataFrames. For each column,\n",
    "    it replaces values that are unique to either set (not common to both) with NaN.\n",
    "\n",
    "    :param train: DataFrame used for training.\n",
    "    :param test: DataFrame used for testing.\n",
    "    :return: Tuple of modified train and test DataFrames.\n",
    "    \"\"\"\n",
    "    aligned_train = train.copy()\n",
    "    aligned_test = test.copy()\n",
    "\n",
    "    for column in train.columns:\n",
    "        if column in ignore_columns:\n",
    "            continue\n",
    "        if column in test.columns:\n",
    "            # Find values that are not common to both train and test sets\n",
    "            common_values = set(train[column]).intersection(set(test[column]))\n",
    "\n",
    "            aligned_train[column] = train[column].apply(lambda x: x if x in common_values else fill_value)\n",
    "            aligned_test[column] = test[column].apply(lambda x: x if x in common_values else fill_value)\n",
    "\n",
    "    return aligned_train, aligned_test\n",
    "\n",
    "\n",
    "def replace_rare_values(df, cols, threshold, replacement_value):\n",
    "    \"\"\"\n",
    "    This function replaces values in each column of the DataFrame that appear less frequently\n",
    "    than the specified threshold with a specified replacement value.\n",
    "\n",
    "    :param df: DataFrame to process.\n",
    "    :param threshold: Frequency threshold. Values appearing less than this will be replaced.\n",
    "    :param replacement_value: The value to replace rare values with.\n",
    "    :return: Modified DataFrame.\n",
    "    \"\"\"\n",
    "    for column in cols:\n",
    "        value_counts = df[column].value_counts()\n",
    "        values_to_replace = value_counts[value_counts < threshold].index\n",
    "\n",
    "        df[column] = df[column].apply(lambda x: replacement_value if x in values_to_replace else x)\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")\n",
    "\n",
    "if CFG.align_train_test:\n",
    "    train_df, test_df = align_train_test_unique(\n",
    "        train=train_df,\n",
    "        test=test_df,\n",
    "        ignore_columns=[\n",
    "            \"uid\",\n",
    "            \"data\",\n",
    "            \"fold\",\n",
    "            \"created_at\",\n",
    "            \"tree_dbh\",\n",
    "            # \"spc_common\",\n",
    "            # \"spc_latin\",\n",
    "        ],\n",
    "    )  #  test ã«ãªã„ã‚‚ã®ã¯ nan ã«ã™ã‚‹\n",
    "\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype == \"float\":\n",
    "        continue\n",
    "    logger.info(f\"{col}: {train_df[col].nunique()}, {test_df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "\n",
    "if CFG.replace_rare_values_threshold is not None:\n",
    "    raw_df = replace_rare_values(\n",
    "        df=raw_df,\n",
    "        cols=[\n",
    "            \"spc_common\",\n",
    "            \"spc_latin\",\n",
    "            \"boro_ct\",\n",
    "            \"cb_num\",\n",
    "            \"st_assem\",\n",
    "            \"nta\",\n",
    "            \"nta_name\",\n",
    "            \"zip_city\",\n",
    "            \"borocode\",\n",
    "            \"boroname\",\n",
    "            \"st_senate\",\n",
    "            \"cncldist\",\n",
    "        ],\n",
    "        threshold=CFG.replace_rare_values_threshold,\n",
    "        replacement_value=np.nan,\n",
    "    )\n",
    "\n",
    "    for col in raw_df.columns:\n",
    "        logger.info(f\"{col}: {raw_df[col].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "for extractor in CFG.rolling_agg_feature_extractors:\n",
    "    if CFG.group_keys_for_rolling_agg is not None:\n",
    "        for group_keys in CFG.group_keys_for_rolling_agg:\n",
    "            _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "            feature_extractors.append(_extractor)\n",
    "\n",
    "for extractor in CFG.agg_feature_extractors:\n",
    "    if CFG.group_keys_for_agg is not None:\n",
    "        for group_keys in CFG.group_keys_for_agg:\n",
    "            _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "            feature_extractors.append(_extractor)\n",
    "\n",
    "\n",
    "raw_feature_df = run_extractors(\n",
    "    input_df=raw_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=CFG.cache_feature_extractors,\n",
    ")\n",
    "assert len(raw_df) == len(raw_feature_df)\n",
    "\n",
    "raw_feature_df = pd.concat([raw_df, raw_feature_df], axis=1)\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True).astype({\"health\": int})\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]\n",
    "logger.info(f\"num features: {len(feature_columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_seed_average_pred(result_df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cols = [col for col in result_df.columns if col.startswith(\"pred_\")]\n",
    "    pred = 0\n",
    "    for col in cols:\n",
    "        pred += np.array(result_df[col].tolist())\n",
    "    result_df[\"pred\"] = list(pred / len(cols))\n",
    "    return result_df.reset_index()\n",
    "\n",
    "\n",
    "valid_result_df = pd.DataFrame()\n",
    "all_trained_estimators = []\n",
    "scores = {}\n",
    "for seed in CFG.seed_average_seeds:\n",
    "    logger.info(f\"\\n\\nstart training seed={seed} ðŸš€\")\n",
    "    CFG.model.estimator.random_state = seed\n",
    "    CFG.model.estimator.num_leaves = seed  # lgbm\n",
    "\n",
    "    if CFG.use_cat_features:\n",
    "        cat_features = [x for x in feature_columns if x.startswith(\"f_oe_\")]\n",
    "        estimator = hydra.utils.instantiate(CFG.model.estimator, cat_features=cat_features)\n",
    "    else:\n",
    "        estimator = hydra.utils.instantiate(CFG.model.estimator)\n",
    "\n",
    "    fit_params = dict(hydra.utils.instantiate(CFG.model.fit_params))\n",
    "    if estimator.__class__.__name__.startswith(\"LGBM\"):\n",
    "        fit_params[\"eval_metric\"] = [\n",
    "            # lgb_macro_auc,\n",
    "            lgb_macro_f1,\n",
    "        ]\n",
    "\n",
    "    model_output_dir = OUTPUT_DIR / \"models\" / f\"seed{seed}\"\n",
    "    trained_estimators = train_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimator=estimator,\n",
    "        feature_columns=feature_columns,\n",
    "        target_columns=[\"health\"],\n",
    "        fit_params=fit_params,\n",
    "        output_dir=model_output_dir,\n",
    "        overwrite=CFG.overwrite_training,\n",
    "    )\n",
    "\n",
    "    i_valid_result_df = predict_cv_tabular_v1(\n",
    "        df=train_feature_df,\n",
    "        estimators=trained_estimators,\n",
    "        feature_columns=feature_columns,\n",
    "        predict_proba=CFG.model.predict_proba,\n",
    "    )\n",
    "    val_score = macro_f1_from_proba(\n",
    "        y_true=i_valid_result_df[\"health\"],\n",
    "        y_pred=i_valid_result_df[\"pred\"].tolist(),\n",
    "    )\n",
    "    logger.info(f\"macro f1 score [seed={seed}]: {val_score}\")\n",
    "    scores[f\"seed{seed}\"] = val_score\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [\n",
    "            valid_result_df,\n",
    "            i_valid_result_df[[\"uid\", \"pred\", \"health\"]]\n",
    "            .set_index([\"uid\", \"health\"])\n",
    "            .rename(columns={\"pred\": f\"pred_{seed}\"}),\n",
    "        ],\n",
    "        axis=1,\n",
    "    )\n",
    "    all_trained_estimators.extend(trained_estimators)\n",
    "\n",
    "\n",
    "valid_result_df = assign_seed_average_pred(valid_result_df)\n",
    "val_proba = np.array(valid_result_df[\"pred\"].tolist())\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=val_proba)\n",
    "scores[\"all_nomal\"] = val_score\n",
    "\n",
    "opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "    proba_matrix=val_proba,\n",
    "    true_labels=valid_result_df[\"health\"],\n",
    "    label_indices=[2, 0, 1],\n",
    ")\n",
    "best_f1_score = f1_score(\n",
    "    y_true=valid_result_df[\"health\"],\n",
    "    y_pred=val_pred_label,\n",
    "    average=\"macro\",\n",
    ")\n",
    "scores[\"all_opt\"] = best_f1_score\n",
    "\n",
    "logger.info(f\"macro f1 score: {val_score}\")\n",
    "logger.info(f\"optimized thresholds: {opt_results}\")\n",
    "logger.info(f\"best f1 score: {best_f1_score}\")\n",
    "\n",
    "joblib.dump(valid_result_df[[\"uid\", \"health\", \"pred\"]], OUTPUT_DIR / \"valid_result_df.pkl\")\n",
    "json.dump(scores, open(OUTPUT_DIR / \"scores.json\", \"w\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")\n",
    "fig.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "importance_df.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)\n",
    "\n",
    "\n",
    "fig = plot_label_distributions(proba_matrix=np.array(valid_result_df[\"pred\"].tolist()))\n",
    "fig.show()\n",
    "fig.savefig(OUTPUT_DIR / \"label_distributions.png\", dpi=300)\n",
    "\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=valid_result_df[\"health\"], y_pred=val_pred_label, normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=all_trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    "    predict_proba=CFG.model.predict_proba,\n",
    ")\n",
    "\n",
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"pred\"] = np.argmax(test_pred_df.groupby(\"uid\").mean(), axis=1)\n",
    "submission_df = test_df[[\"uid\", \"pred\"]]\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "test_df[\"opt_pred\"] = decode_label(proba_matrix=test_pred_df.groupby(\"uid\").mean().to_numpy(), thresholds=opt_results)\n",
    "\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_opt_{best_f1_score:.3f}.csv\"\n",
    "test_df[[\"uid\", \"opt_pred\"]].to_csv(submission_filepath, index=False, header=False)\n",
    "\n",
    "joblib.dump(test_pred_df, OUTPUT_DIR / \"test_result_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_df[\"opt_pred\"].value_counts() / len(test_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"pred\"].value_counts() / len(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"health\"].value_counts() / len(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_dist_by_color(df, value_col, color_col):\n",
    "    import seaborn as sns\n",
    "    import matplotlib.pyplot as plt\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    for color in df[color_col].unique():\n",
    "        sns.distplot(df[value_col][df[color_col] == color], hist=False, label=color)\n",
    "\n",
    "    plt.title(\"Distribution of Values Color-Coded by Color\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
