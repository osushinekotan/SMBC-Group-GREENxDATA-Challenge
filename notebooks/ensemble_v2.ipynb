{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.experiment.optimization import find_optimal_threshold_for_label, decode_label\n",
    "from src.experiment.model.custom_metrics import lgb_macro_auc, lgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=057-ensemble_v2\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)\n",
    "BASE_OUTPUT_DIR = Path(CFG.paths.resource_dir) / \"outputs\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pred_df(ensemble_exps):\n",
    "    train_pred_df = pd.DataFrame()\n",
    "    test_pred_df = pd.DataFrame()\n",
    "    for exp in ensemble_exps:\n",
    "        logger.info(f\"Load {exp} ...\")\n",
    "        train_filepath = BASE_OUTPUT_DIR / exp / \"valid_result_df.pkl\"\n",
    "        test_filepath = BASE_OUTPUT_DIR / exp / \"test_result_df.pkl\"\n",
    "\n",
    "        train_result_df = joblib.load(train_filepath).set_index(\"uid\")\n",
    "        test_result_df = joblib.load(test_filepath)\n",
    "\n",
    "        test_pred_df_ = test_result_df[[\"uid\", 0, 1, 2]].groupby(\"uid\").mean()\n",
    "        test_pred_df_[\"pred\"] = test_pred_df_.to_numpy().tolist()\n",
    "\n",
    "        train_pred_df = pd.concat([train_pred_df, train_result_df[[\"pred\"]].add_prefix(f\"{exp}_\")], axis=1)\n",
    "        test_pred_df = pd.concat([test_pred_df, test_pred_df_[[\"pred\"]].add_prefix(f\"{exp}_\")], axis=1)\n",
    "\n",
    "    return train_pred_df.reset_index(), test_pred_df.reset_index()\n",
    "\n",
    "\n",
    "train_pred_df, test_pred_df = get_pred_df(CFG.ensemble_exps)\n",
    "train_pred_df = pd.merge(train_df[[\"uid\", \"health\"]], train_pred_df, on=\"uid\")\n",
    "pred_cols = [col for col in train_pred_df.columns if col.endswith(\"pred\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def macro_f1_score(weights, preds, true_labels):\n",
    "    weighted_preds = np.average(preds, axis=1, weights=weights)\n",
    "    labels_pred = np.argmax(weighted_preds, axis=1)\n",
    "    return -f1_score(true_labels, labels_pred, average=\"macro\")  # 最大化のために負にする\n",
    "\n",
    "\n",
    "def optimize_weights(preds, true_labels):\n",
    "    num_predictors = preds.shape[1]  # 予測器の数\n",
    "    cons = {\"type\": \"eq\", \"fun\": lambda w: np.sum(w) - 1}\n",
    "    initial_weights = np.full(num_predictors, 1 / num_predictors)\n",
    "    bounds = [(0, 1) for _ in range(num_predictors)]\n",
    "    result = minimize(\n",
    "        macro_f1_score,\n",
    "        initial_weights,\n",
    "        args=(preds, true_labels),\n",
    "        method=\"SLSQP\",\n",
    "        bounds=bounds,\n",
    "        constraints=cons,\n",
    "    )\n",
    "\n",
    "    return result.x if result.success else None\n",
    "\n",
    "\n",
    "def opt_decode_preds(preds, label):\n",
    "    opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "        proba_matrix=preds,\n",
    "        true_labels=label,\n",
    "        label_indices=[2, 0, 1],\n",
    "    )\n",
    "\n",
    "    best_f1_score = f1_score(\n",
    "        y_true=label,\n",
    "        y_pred=val_pred_label,\n",
    "        average=\"macro\",\n",
    "    )\n",
    "    return best_f1_score, opt_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.array(train_pred_df[pred_cols].to_numpy().tolist())\n",
    "label = train_pred_df[\"health\"].to_numpy()\n",
    "\n",
    "weights = CFG.get(\"ensemble_weights\")\n",
    "if weights is None:\n",
    "    weights = optimize_weights(preds, label)\n",
    "\n",
    "assert len(weights) == preds.shape[1]\n",
    "\n",
    "weighted_preds = np.average(preds, axis=1, weights=weights)\n",
    "\n",
    "best_f1_score, opt_results = opt_decode_preds(weighted_preds, label)\n",
    "logger.info(f\"weights: {weights}\")\n",
    "logger.info(f\"Best F1 Score: {best_f1_score}\")\n",
    "train_pred_df[\"pred_label\"] = decode_label(proba_matrix=weighted_preds, thresholds=opt_results)\n",
    "score = f1_score(y_pred=train_pred_df[\"pred_label\"], y_true=label, average=\"macro\")\n",
    "logger.info(f\"Macro F1 Score: {score}\")\n",
    "\n",
    "test_preds = np.array(test_pred_df[pred_cols].to_numpy().tolist())\n",
    "test_pred_df[\"pred\"] = np.average(test_preds, axis=1, weights=weights).tolist()\n",
    "test_pred_df[\"pred_label\"] = decode_label(\n",
    "    proba_matrix=np.array(test_pred_df[\"pred\"].to_numpy().tolist()),\n",
    "    thresholds=opt_results,\n",
    ")\n",
    "\n",
    "result_dict = {\"weights\": list(weights), \"opt_results\": opt_results, \"best_f1_score\": best_f1_score}\n",
    "json.dump(result_dict, open(OUTPUT_DIR / \"result.json\", \"w\"), indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_confusion_matrix(y_true=train_pred_df[\"health\"], y_pred=train_pred_df[\"pred_label\"])\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=train_pred_df[\"health\"], y_pred=train_pred_df[\"pred_label\"], normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_pred_df[[\"uid\", \"pred_label\"]].sort_values(\"uid\")\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_{score:.3f}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
