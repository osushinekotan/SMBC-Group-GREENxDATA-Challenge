{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import sklearn\n",
    "import rootutils\n",
    "import re\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "import category_encoders as ce\n",
    "from xgboost import XGBModel\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import assign_fold_index, make_uid, visualize_feature_importance\n",
    "from src.experiment.base import BaseFeatureExtractor\n",
    "from src.experiment.runner import run_extractors\n",
    "from src.experiment.feature.tabular import AggregatedFeatureExtractor\n",
    "from src.experiment.metrics import macro_f1_from_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=001-tabular_v01\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")\n",
    "\n",
    "test_df[\"fold\"] = -1\n",
    "train_df[\"data\"] = \"train\"\n",
    "test_df[\"data\"] = \"test\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CreatedAtFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def transform(self, input_df):\n",
    "        ts = pd.to_datetime(input_df[\"created_at\"])\n",
    "\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df = output_df.assign(\n",
    "            created_at__year=ts.dt.year,\n",
    "            created_at__month=ts.dt.month,\n",
    "            created_at__day=ts.dt.day,\n",
    "        )\n",
    "\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class CurbLocationFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self):\n",
    "        self.mapping = {\"OnCurb\": 1, \"OffsetFromCurb\": 0}\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame({\"curb_loc_binary\": input_df[\"curb_loc\"].map(self.mapping).tolist()})\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class StreetWidthFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self):\n",
    "        self.mapping = {\"1or2\": 0, \"3or4\": 1, \"4orMore\": 2}\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame({\"steward_rank\": input_df[\"steward\"].map(self.mapping).tolist()})\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class GuardsFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self):\n",
    "        self.mapping = {\"Helpful\": 0, \"Unsure\": 1, \"Harmful\": 2}\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame({\"guards_rank\": input_df[\"guards\"].map(self.mapping).tolist()})\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class SidewalkFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self) -> None:\n",
    "        self.mapping = {\"NoDamage\": 0, \"Damage\": 1}\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = pd.DataFrame({\"sidewalk_binary\": input_df[\"sidewalk\"].map(self.mapping).tolist()})\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class UserTypeFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self) -> None:\n",
    "        self.mapping = {\"Volunteer\": 0, \"TreesCount Staff\": 1, \"NYC Parks Staff\": 2}\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        user_types = input_df[\"user_type\"].map(self.mapping)\n",
    "\n",
    "        output_df = pd.DataFrame()\n",
    "        output_df[\"user_type_rank\"] = input_df[\"user_type\"].map(self.mapping)\n",
    "        output_df[\"is_volunteer\"] = (user_types == 0).tolist()\n",
    "\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class ProblemsFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def make_num_problems_df(self, problems: pd.Series | list[str]) -> list:\n",
    "        num_problems = [len(re.split(\"(?=[A-Z])\", problem)[1:]) if problem != \"nan\" else np.nan for problem in problems]\n",
    "        return pd.DataFrame({\"num_problems\": num_problems})\n",
    "\n",
    "    def make_problems_onehot_df(self, input_df) -> pd.DataFrame:\n",
    "        df = input_df[[\"problems\"]].copy()\n",
    "        for index, item in df[[\"problems\"]].fillna(\"Nan\").iterrows():\n",
    "            elements = re.split(\"(?=[A-Z])\", item[\"problems\"])\n",
    "            for element in elements:\n",
    "                if element:\n",
    "                    df.at[index, element] = 1\n",
    "            if \"Other\" in item:\n",
    "                df.at[index, \"Other\"] = 1\n",
    "        return df.drop(columns=[\"problems\"]).fillna(0).astype(int).add_prefix(\"problem_is_\")\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        features_num_problems_df = self.make_num_problems_df(input_df[\"problems\"].fillna(\"nan\"))\n",
    "        features_problems_onehot_df = self.make_problems_onehot_df(input_df)\n",
    "        output_df = pd.concat([features_num_problems_df, features_problems_onehot_df], axis=1)\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class NtaFeatureExtractorV1(BaseFeatureExtractor):\n",
    "    def __init__(self) -> None:\n",
    "        self.oe = ce.OrdinalEncoder()\n",
    "\n",
    "    def fit(self, input_df):\n",
    "        df = self.parse_nta(input_df)\n",
    "        self.oe.fit(df)\n",
    "        return self\n",
    "\n",
    "    def parse_nta(self, input_df):\n",
    "        df = input_df[[\"nta\"]].copy()\n",
    "        df[\"nta_char\"] = df[\"nta\"].str[:2]\n",
    "        df[\"nta_num\"] = df[\"nta\"].str[2:]\n",
    "        return df\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        df = self.parse_nta(X)\n",
    "        output_df = self.oe.transform(df).add_prefix(\"oe_\")\n",
    "        return output_df\n",
    "\n",
    "\n",
    "class RawTransformer(BaseFeatureExtractor):\n",
    "    def __init__(self, cols: list[str]) -> None:\n",
    "        self.cols = cols\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = input_df[self.cols].copy()\n",
    "        return output_df.astype(np.float32)\n",
    "\n",
    "\n",
    "class OrdinalFeatureExtractor(BaseFeatureExtractor):\n",
    "    def __init__(self, cols=list[str]) -> None:\n",
    "        self.cols = cols\n",
    "        self.oe = ce.OrdinalEncoder()\n",
    "\n",
    "    def fit(self, input_df):\n",
    "        self.oe.fit(input_df[self.cols].astype(str))\n",
    "        return self\n",
    "\n",
    "    def transform(self, input_df):\n",
    "        output_df = self.oe.transform(input_df[self.cols].astype(str)).add_prefix(\"oe_\")\n",
    "        return output_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = [\n",
    "    CreatedAtFeatureExtractorV1(),\n",
    "    CurbLocationFeatureExtractorV1(),\n",
    "    StreetWidthFeatureExtractorV1(),\n",
    "    GuardsFeatureExtractorV1(),\n",
    "    SidewalkFeatureExtractorV1(),\n",
    "    UserTypeFeatureExtractorV1(),\n",
    "    ProblemsFeatureExtractorV1(),\n",
    "    NtaFeatureExtractorV1(),\n",
    "    RawTransformer(\n",
    "        cols=[\n",
    "            \"tree_dbh\",\n",
    "            \"cb_num\",\n",
    "            \"st_senate\",\n",
    "            \"st_assem\",\n",
    "            \"cncldist\",\n",
    "            \"borocode\",\n",
    "        ]\n",
    "    ),\n",
    "    OrdinalFeatureExtractor(\n",
    "        cols=[\n",
    "            \"spc_common\",\n",
    "            \"spc_latin\",\n",
    "            \"cncldist\",\n",
    "            \"boro_ct\",\n",
    "            \"boroname\",\n",
    "            \"zip_city\",\n",
    "        ]\n",
    "    ),\n",
    "    AggregatedFeatureExtractor(\n",
    "        group_keys=[\"created_at__year\"],\n",
    "        group_values=[\"tree_dbh\"],\n",
    "        agg_methods=[\"min\", \"max\", \"std\", \"mean\", \"median\"],\n",
    "        extr_agg_methods=[\"z-score\"],\n",
    "        parents=[CreatedAtFeatureExtractorV1()],\n",
    "    ),\n",
    "    *[\n",
    "        AggregatedFeatureExtractor(\n",
    "            group_keys=keys,\n",
    "            group_values=[\"tree_dbh\"],\n",
    "            agg_methods=[\"min\", \"max\", \"std\", \"mean\", \"median\"],\n",
    "            extr_agg_methods=[\"z-score\"],\n",
    "        )\n",
    "        for keys in [\n",
    "            [\"sidewalk\"],\n",
    "            [\"problems\"],\n",
    "            [\"zip_city\"],\n",
    "            [\"steward\"],\n",
    "            [\"cb_num\"],\n",
    "            [\"boroname\"],\n",
    "            [\"sidewalk\", \"spc_common\"],\n",
    "            [\"problems\", \"spc_common\"],\n",
    "            [\"zip_city\", \"spc_common\"],\n",
    "            [\"cb_num\", \"spc_common\"],\n",
    "            [\"boroname\", \"spc_common\"],\n",
    "            [\"steward\", \"spc_common\"],\n",
    "            [\"steward\", \"steward\"],\n",
    "            [\"steward\", \"problems\"],\n",
    "            [\"steward\", \"zip_city\"],\n",
    "            [\"steward\", \"cb_num\"],\n",
    "            [\"steward\", \"boroname\"],\n",
    "        ]\n",
    "    ],\n",
    "]\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "raw_feature_df = run_extractors(\n",
    "    input_df=raw_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.output_dir) / \"features\",\n",
    "    fit=True,\n",
    "    cache=True,\n",
    ")\n",
    "\n",
    "raw_feature_df = pd.concat([raw_df, raw_feature_df], axis=1)\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True)\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_tabular_v1(\n",
    "    df: pd.DataFrame,\n",
    "    estimator,\n",
    "    feature_columns: list[str],\n",
    "    target_columns: str,\n",
    "    fit_params: dict,\n",
    "    output_dir: Path,\n",
    "    train_folds: list[int] | None = None,\n",
    "    overwrite: bool = False,\n",
    "    use_sample_weight: bool = False,\n",
    "):\n",
    "    \"\"\"train cv for xgboost estimator\"\"\"\n",
    "    estimators = []\n",
    "\n",
    "    if train_folds is None:\n",
    "        train_folds = sorted(df[\"fold\"].unique())\n",
    "\n",
    "    for i_fold in train_folds:\n",
    "        logger.info(f\"start training fold={i_fold} ðŸš€ \")\n",
    "        fit_estimator = clone(estimator)\n",
    "\n",
    "        output_df_fold = output_dir / f\"fold{i_fold}\"\n",
    "        output_df_fold.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        estimator_uid = make_uid(fit_estimator.__dict__)\n",
    "        estimator_name = f\"{fit_estimator.__class__.__name__}_{estimator_uid}\"\n",
    "        estimator_path = output_df_fold / f\"{estimator_name}.pkl\"\n",
    "\n",
    "        if estimator_path.exists() and (not overwrite):\n",
    "            logger.info(f\"skip fitting in fold{i_fold}\")\n",
    "            fit_estimator = joblib.load(estimator_path)\n",
    "            estimators.append(fit_estimator)\n",
    "            continue\n",
    "\n",
    "        # split train and valid\n",
    "        train_df = df.query(f\"fold != {i_fold}\").reset_index(drop=True)\n",
    "        valid_df = df.query(f\"fold == {i_fold}\").reset_index(drop=True)\n",
    "        tr_x, tr_y = train_df[feature_columns], train_df[target_columns]\n",
    "        va_x, va_y = valid_df[feature_columns], valid_df[target_columns]\n",
    "\n",
    "        logger.info(f\"estimator : {estimator_name}\")\n",
    "\n",
    "        if use_sample_weight:\n",
    "            fit_params[\"sample_weight\"] = compute_sample_weight(class_weight=\"balanced\", y=tr_y)\n",
    "            fit_params[\"sample_weight_eval_set\"] = [compute_sample_weight(class_weight=\"balanced\", y=va_y)]\n",
    "\n",
    "        fit_estimator.fit(X=tr_x, y=tr_y, eval_set=[(va_x, va_y)], **fit_params)\n",
    "        estimators.append(fit_estimator)\n",
    "\n",
    "        joblib.dump(fit_estimator, estimator_path)\n",
    "\n",
    "    return estimators\n",
    "\n",
    "\n",
    "def predict_cv_tabular_v1(\n",
    "    df: pd.DataFrame,\n",
    "    estimators: list,\n",
    "    feature_columns: list[str],\n",
    "    train_folds: list[int] | None = None,\n",
    "    test: bool = False,\n",
    "    result_columns: list[str] | None = None,\n",
    "):\n",
    "    if result_columns is None:\n",
    "        result_columns = [col for col in df.columns if col not in feature_columns]\n",
    "\n",
    "    if train_folds is None:\n",
    "        train_folds = sorted(df[\"fold\"].unique())\n",
    "\n",
    "    def _predict_i(df, i_fold, estimator):\n",
    "        logger.info(f\"fold{i_fold} predict : test={test}\")\n",
    "        if not test:\n",
    "            df = df.query(f\"fold == {i_fold}\").reset_index(drop=True)\n",
    "\n",
    "        va_x = df[feature_columns]\n",
    "        va_pred = estimator.predict(va_x)\n",
    "        i_result_df = df[result_columns].assign(pred=va_pred.tolist())\n",
    "        return i_result_df\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [_predict_i(df, i_fold, estimator) for i_fold, estimator in zip(train_folds, estimators)],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_params = {\n",
    "    # https://xgboost.readthedocs.io/en/stable/parameter.html#:~:text=Learning%20Task%20Parameters-,%EF%83%81,-Specify%20the%20learning\n",
    "    \"device\": \"gpu\",\n",
    "    \"n_estimators\": 10000,\n",
    "    \"max_depth\": 8,\n",
    "    \"learning_rate\": 0.01,\n",
    "    \"objective\": \"multi:softprob\",\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"tree_method\": \"hist\",\n",
    "    \"colsample_bytree\": 0.2,\n",
    "    \"subsample\": 0.2,\n",
    "    \"random_state\": 8823,\n",
    "    \"early_stopping_rounds\": 100,\n",
    "    \"num_class\": 3,\n",
    "    \"importance_type\": \"gain\",\n",
    "}\n",
    "\n",
    "fit_params = {\"verbose\": 100}\n",
    "\n",
    "estimator = XGBModel(**model_params)\n",
    "model_output_dir = Path(CFG.paths.output_dir) / \"models\"\n",
    "\n",
    "trained_estimators = train_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimator=estimator,\n",
    "    feature_columns=feature_columns,\n",
    "    target_columns=[\"health\"],\n",
    "    fit_params=fit_params,\n",
    "    output_dir=model_output_dir,\n",
    "    overwrite=True,\n",
    "    use_sample_weight=True,\n",
    ")\n",
    "\n",
    "valid_result_df = predict_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    ")\n",
    "\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=valid_result_df[\"pred\"].tolist())\n",
    "logger.info(f\"macro f1 score: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
