{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import json\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import warnings\n",
    "import rootutils\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import (\n",
    "    assign_fold_index,\n",
    "    plot_confusion_matrix,\n",
    "    visualize_feature_importance,\n",
    "    plot_label_distributions,\n",
    ")\n",
    "from src.experiment.feature.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1\n",
    "from src.experiment.optimization import find_optimal_threshold_for_label, decode_label\n",
    "from src.experiment.model.custom_metrics import lgb_macro_auc, lgb_macro_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=089-ensemble_v1\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)\n",
    "BASE_OUTPUT_DIR = Path(CFG.paths.resource_dir) / \"outputs\"\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_pred_label_df(ensemble_exps: list[str]):\n",
    "    feature_df = pd.DataFrame()\n",
    "    for exp in ensemble_exps:\n",
    "        logger.info(f\"Load {exp} ...\")\n",
    "        train_filepath = BASE_OUTPUT_DIR / exp / \"valid_result_df.pkl\"\n",
    "        test_filepath = BASE_OUTPUT_DIR / exp / \"test_result_df.pkl\"\n",
    "\n",
    "        train_result_df = joblib.load(train_filepath)\n",
    "        test_result_df = joblib.load(test_filepath)\n",
    "\n",
    "        test_pred_df_ = test_result_df[[\"uid\", 0, 1, 2]].groupby(\"uid\").mean()\n",
    "        test_pred_df_[\"pred\"] = test_pred_df_.to_numpy().tolist()\n",
    "        test_pred_df = test_pred_df_[[\"pred\"]].reset_index()\n",
    "        opt_results, val_pred_label = find_optimal_threshold_for_label(\n",
    "            proba_matrix=np.array(train_result_df[\"pred\"].to_numpy().tolist()),\n",
    "            true_labels=train_result_df[\"health\"],\n",
    "            label_indices=[2, 0, 1],\n",
    "        )\n",
    "\n",
    "        best_f1_score = f1_score(\n",
    "            y_true=train_result_df[\"health\"],\n",
    "            y_pred=val_pred_label,\n",
    "            average=\"macro\",\n",
    "        )\n",
    "\n",
    "        logger.info(f\"best f1 score: {best_f1_score:.4f}\")\n",
    "\n",
    "        train_pred_df = (\n",
    "            train_result_df[[\"uid\"]]\n",
    "            .assign(pred_label=val_pred_label)\n",
    "            .set_index(\"uid\")\n",
    "            .add_prefix(f\"{exp}_\")\n",
    "            .reset_index()\n",
    "        )\n",
    "        test_pred_df[\"pred_label\"] = decode_label(\n",
    "            proba_matrix=np.array(test_pred_df[\"pred\"].to_numpy().tolist()), thresholds=opt_results\n",
    "        )\n",
    "        test_pred_df = test_pred_df[[\"uid\", \"pred_label\"]].set_index(\"uid\").add_prefix(f\"{exp}_\").reset_index()\n",
    "\n",
    "        df = pd.concat(\n",
    "            [train_pred_df.set_index(\"uid\"), test_pred_df.set_index(\"uid\")],\n",
    "            axis=0,\n",
    "            ignore_index=False,\n",
    "        )\n",
    "        feature_df = pd.concat([feature_df, df], axis=1)\n",
    "\n",
    "    return feature_df\n",
    "\n",
    "\n",
    "def voting(candidates: list[int], priorities: list[int]):\n",
    "    vote_counts = {candidate: candidates.count(candidate) for candidate in set(candidates)}\n",
    "    max_votes = max(vote_counts.values())\n",
    "    potential_winners = [candidate for candidate, votes in vote_counts.items() if votes == max_votes]\n",
    "\n",
    "    if len(potential_winners) == 1:\n",
    "        return potential_winners[0]\n",
    "    for priority in priorities:\n",
    "        if priority in potential_winners:\n",
    "            return priority\n",
    "\n",
    "\n",
    "ensemble_pred_df = make_pred_label_df(ensemble_exps=CFG.ensemble_exps)\n",
    "ensemble_pred_df[\"pred_label\"] = ensemble_pred_df.apply(\n",
    "    lambda row: voting(row.tolist(), priorities=[2, 0, 1]), axis=1\n",
    ").tolist()\n",
    "raw_feature_df = pd.merge(raw_df, ensemble_pred_df, on=\"uid\", how=\"left\")\n",
    "\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True).astype({\"health\": int})\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "score = f1_score(\n",
    "    y_true=train_feature_df[\"health\"],\n",
    "    y_pred=train_feature_df[\"pred_label\"],\n",
    "    average=\"macro\",\n",
    ")\n",
    "logger.info(f\"score: {score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plot_confusion_matrix(y_true=train_feature_df[\"health\"], y_pred=train_feature_df[\"pred_label\"])\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix.png\", dpi=300)\n",
    "\n",
    "fig = plot_confusion_matrix(y_true=train_feature_df[\"health\"], y_pred=train_feature_df[\"pred_label\"], normalize=True)\n",
    "fig.savefig(OUTPUT_DIR / \"confusion_matrix_normalized.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = test_feature_df[[\"uid\", \"pred_label\"]].sort_values(\"uid\")\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}_{score:.3f}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df[\"pred_label\"].value_counts() / len(submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
