{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import rootutils\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from sklearn.base import clone\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import assign_fold_index, make_uid, visualize_feature_importance\n",
    "from src.experiment.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=002-tabular_v2\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "\n",
    "for extractor in CFG.agg_feature_extractors:\n",
    "    for group_keys in CFG.group_keys_for_agg:\n",
    "        _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "        feature_extractors.append(_extractor)\n",
    "\n",
    "for extractor in CFG.te_feature_extractors:\n",
    "    for group_keys in CFG.group_keys_for_te:\n",
    "        _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "        feature_extractors.append(_extractor)\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "raw_feature_df = run_extractors(\n",
    "    input_df=raw_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=False,\n",
    ")\n",
    "assert len(raw_df) == len(raw_feature_df)\n",
    "\n",
    "raw_feature_df = pd.concat([raw_df, raw_feature_df], axis=1)\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True)\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_cv_tabular_v1(\n",
    "    df: pd.DataFrame,\n",
    "    estimator,\n",
    "    feature_columns: list[str],\n",
    "    target_columns: str,\n",
    "    fit_params: dict,\n",
    "    output_dir: Path,\n",
    "    train_folds: list[int] | None = None,\n",
    "    overwrite: bool = False,\n",
    "    use_xgb_class_weight: bool = False,\n",
    "):\n",
    "    \"\"\"train cv for xgboost estimator\"\"\"\n",
    "    estimators = []\n",
    "\n",
    "    if train_folds is None:\n",
    "        train_folds = sorted(df[\"fold\"].unique())\n",
    "\n",
    "    for i_fold in train_folds:\n",
    "        logger.info(f\"start training fold={i_fold} ðŸš€ \")\n",
    "        fit_estimator = clone(estimator)\n",
    "\n",
    "        output_df_fold = output_dir / f\"fold{i_fold}\"\n",
    "        output_df_fold.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "        estimator_uid = make_uid(fit_estimator.__dict__)\n",
    "        estimator_name = fit_estimator.__class__.__name__\n",
    "        estimator_name_with_uid = f\"{estimator_name}_{estimator_uid}\"\n",
    "        estimator_path = output_df_fold / f\"{estimator_name}.pkl\"\n",
    "\n",
    "        if estimator_path.exists() and (not overwrite):\n",
    "            logger.info(f\"skip fitting in fold{i_fold}\")\n",
    "            fit_estimator = joblib.load(estimator_path)\n",
    "            estimators.append(fit_estimator)\n",
    "            continue\n",
    "\n",
    "        # split train and valid\n",
    "        train_df = df.query(f\"fold != {i_fold}\").reset_index(drop=True)\n",
    "        valid_df = df.query(f\"fold == {i_fold}\").reset_index(drop=True)\n",
    "        tr_x, tr_y = train_df[feature_columns], train_df[target_columns]\n",
    "        va_x, va_y = valid_df[feature_columns], valid_df[target_columns]\n",
    "\n",
    "        logger.info(f\"estimator : {estimator_name_with_uid}\")\n",
    "\n",
    "        if use_xgb_class_weight:\n",
    "            if estimator_name == \"XGBModel\":\n",
    "                fit_params[\"sample_weight\"] = compute_sample_weight(class_weight=\"balanced\", y=tr_y)\n",
    "                fit_params[\"sample_weight_eval_set\"] = [compute_sample_weight(class_weight=\"balanced\", y=va_y)]\n",
    "\n",
    "        fit_estimator.fit(X=tr_x, y=tr_y, eval_set=[(va_x, va_y)], **fit_params)\n",
    "        estimators.append(fit_estimator)\n",
    "\n",
    "        joblib.dump(fit_estimator, estimator_path)\n",
    "\n",
    "    return estimators\n",
    "\n",
    "\n",
    "def predict_cv_tabular_v1(\n",
    "    df: pd.DataFrame,\n",
    "    estimators: list,\n",
    "    feature_columns: list[str],\n",
    "    train_folds: list[int] | None = None,\n",
    "    test: bool = False,\n",
    "    result_columns: list[str] | None = None,\n",
    "):\n",
    "    if result_columns is None:\n",
    "        result_columns = [col for col in df.columns if col not in feature_columns]\n",
    "\n",
    "    if train_folds is None:\n",
    "        train_folds = range(len(estimators))\n",
    "\n",
    "    def _predict_i(df, i_fold, estimator):\n",
    "        logger.info(f\"fold{i_fold} predict : test={test}\")\n",
    "        if not test:\n",
    "            df = df.query(f\"fold == {i_fold}\").reset_index(drop=True)\n",
    "\n",
    "        va_x = df[feature_columns]\n",
    "        va_pred = estimator.predict(va_x)\n",
    "        i_result_df = df[result_columns].assign(pred=va_pred.tolist())\n",
    "        if test:\n",
    "            i_result_df = i_result_df.assign(est_fold=i_fold)\n",
    "        return i_result_df\n",
    "\n",
    "    valid_result_df = pd.concat(\n",
    "        [_predict_i(df, i_fold, estimator) for i_fold, estimator in zip(train_folds, estimators)],\n",
    "        axis=0,\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    return valid_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMModel\n",
    "import lightgbm\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "def lgb_macro_auc(y_true, y_pred):\n",
    "    auc = roc_auc_score(y_true=y_true, y_score=y_pred, multi_class=\"ovr\")\n",
    "    return \"macro_auc\", auc, True\n",
    "\n",
    "\n",
    "def lgb_macro_f1(y_true, y_pred):\n",
    "    f1 = macro_f1_from_proba(y_true=y_true, y_pred=y_pred)\n",
    "    return \"macro_f1\", f1, True\n",
    "\n",
    "\n",
    "model_params = {\n",
    "    # https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMModel.html#lightgbm.LGBMModel\n",
    "    \"n_estimators\": 10000,\n",
    "    \"num_leaves\": 11,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"colsample_bytree\": 0.2,\n",
    "    \"subsample\": 0.2,\n",
    "    \"random_state\": 8823,\n",
    "    # \"class_weight\": \"balanced\",\n",
    "    \"importance_type\": \"gain\",\n",
    "    \"num_class\": 3,\n",
    "}\n",
    "eary_stopping = lightgbm.early_stopping(500, first_metric_only=False, verbose=True)\n",
    "log_evaluation = lightgbm.log_evaluation(period=100, show_stdv=True)\n",
    "\n",
    "fit_params = {\"callbacks\": [eary_stopping, log_evaluation], \"eval_metric\": [lgb_macro_f1]}\n",
    "\n",
    "estimator = LGBMModel(**model_params)\n",
    "model_output_dir = Path(CFG.paths.output_dir) / \"models\"\n",
    "\n",
    "trained_estimators = train_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimator=estimator,\n",
    "    feature_columns=feature_columns,\n",
    "    target_columns=[\"health\"],\n",
    "    fit_params=fit_params,\n",
    "    output_dir=model_output_dir,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "valid_result_df = predict_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    ")\n",
    "\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=valid_result_df[\"pred\"].tolist())\n",
    "logger.info(f\"macro f1 score: {val_score}\")\n",
    "\n",
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(test_result_df):\n",
    "    test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "    test_df[\"pred\"] = np.argmax(test_pred_df.groupby(\"uid\").mean(), axis=1)\n",
    "    return test_df[[\"uid\", \"pred\"]]\n",
    "\n",
    "\n",
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    ")\n",
    "\n",
    "submission_df = make_submission(test_result_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)\n",
    "\n",
    "submission_df_ = pd.read_csv(submission_filepath)\n",
    "assert len(submission_df_) == len(sample_submission_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
