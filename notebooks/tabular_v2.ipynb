{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import hydra\n",
    "import logging\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import rootutils\n",
    "import joblib\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from hydra import compose, initialize\n",
    "from hydra.core.hydra_config import HydraConfig\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "rootutils.setup_root(search_from=\"../\", indicator=\".project-root\", pythonpath=True)\n",
    "\n",
    "from src.experiment.utils import assign_fold_index, make_uid, visualize_feature_importance\n",
    "from src.experiment.runner import run_extractors\n",
    "from src.experiment.metrics import macro_f1_from_proba\n",
    "from src.experiment.model.runner import train_cv_tabular_v1, predict_cv_tabular_v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OVERRIDES: list[str] = os.getenv(\"OVERRIDES\", \"experiment=002-tabular_v2\").split(\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if OVERRIDES is None:\n",
    "    raise ValueError(\"OVERRIDES is not set\")\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../configs\"):\n",
    "    CFG = compose(\n",
    "        config_name=\"config.yaml\",\n",
    "        return_hydra_config=True,\n",
    "        overrides=OVERRIDES,\n",
    "    )\n",
    "    HydraConfig.instance().set_config(CFG)  # use HydraConfig for notebook to use hydra job\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "\n",
    "if not logger.handlers:\n",
    "    handler = logging.StreamHandler()\n",
    "    logger.addHandler(handler)\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "INPUT_DIR = Path(CFG.paths.input_dir)\n",
    "OUTPUT_DIR = Path(CFG.paths.output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_meta(df: pd.DataFrame, data=\"train\"):\n",
    "    df[\"data\"] = data\n",
    "    df[\"fold\"] = -1\n",
    "    return df\n",
    "\n",
    "\n",
    "train_df = pd.read_csv(INPUT_DIR / \"train.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "test_df = pd.read_csv(INPUT_DIR / \"test.csv\").rename(columns={\"Unnamed: 0\": \"uid\"})\n",
    "sample_submission_df = pd.read_csv(INPUT_DIR / \"sample_submission.csv\")\n",
    "\n",
    "train_df = assign_meta(train_df, data=\"train\")\n",
    "test_df = assign_meta(test_df, data=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CV Split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfold = hydra.utils.instantiate(CFG.cv)\n",
    "train_df = assign_fold_index(train_df=train_df, kfold=kfold, y_col=\"health\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Engineering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractors = hydra.utils.instantiate(CFG.feature_extractors)\n",
    "\n",
    "for extractor in CFG.agg_feature_extractors:\n",
    "    for group_keys in CFG.group_keys_for_agg:\n",
    "        _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "        feature_extractors.append(_extractor)\n",
    "\n",
    "for extractor in CFG.te_feature_extractors:\n",
    "    for group_keys in CFG.group_keys_for_te:\n",
    "        _extractor = hydra.utils.instantiate(extractor, group_keys=group_keys)\n",
    "        feature_extractors.append(_extractor)\n",
    "\n",
    "raw_df = pd.concat([train_df, test_df], axis=0, ignore_index=True)\n",
    "raw_feature_df = run_extractors(\n",
    "    input_df=raw_df,\n",
    "    extractors=feature_extractors,\n",
    "    dirpath=Path(CFG.paths.feature_store_dir),\n",
    "    fit=True,\n",
    "    cache=False,\n",
    ")\n",
    "assert len(raw_df) == len(raw_feature_df)\n",
    "\n",
    "raw_feature_df = pd.concat([raw_df, raw_feature_df], axis=1)\n",
    "train_feature_df = raw_feature_df.query(\"data == 'train'\").reset_index(drop=True)\n",
    "test_feature_df = raw_feature_df.query(\"data == 'test'\").reset_index(drop=True)\n",
    "\n",
    "feature_columns = [col for col in train_feature_df.columns if col.startswith(\"f_\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = hydra.utils.instantiate(CFG.estimator)\n",
    "fit_params = hydra.utils.instantiate(CFG.fit_params)\n",
    "\n",
    "model_output_dir = OUTPUT_DIR / \"models\"\n",
    "trained_estimators = train_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimator=estimator,\n",
    "    feature_columns=feature_columns,\n",
    "    target_columns=[\"health\"],\n",
    "    fit_params=fit_params,\n",
    "    output_dir=model_output_dir,\n",
    "    overwrite=True,\n",
    ")\n",
    "\n",
    "valid_result_df = predict_cv_tabular_v1(\n",
    "    df=train_feature_df,\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    ")\n",
    "\n",
    "val_score = macro_f1_from_proba(y_true=valid_result_df[\"health\"], y_pred=valid_result_df[\"pred\"].tolist())\n",
    "logger.info(f\"macro f1 score: {val_score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, importance_df = visualize_feature_importance(\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    top_n=50,\n",
    ")\n",
    "fig.savefig(OUTPUT_DIR / \"feature_importance.png\", dpi=300)\n",
    "importance_df.to_csv(OUTPUT_DIR / \"feature_importance.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make submission\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_submission(test_result_df):\n",
    "    test_pred_df = pd.concat([test_result_df[[\"uid\"]], pd.DataFrame(test_result_df[\"pred\"].tolist())], axis=1)\n",
    "    test_df[\"pred\"] = np.argmax(test_pred_df.groupby(\"uid\").mean(), axis=1)\n",
    "    return test_df[[\"uid\", \"pred\"]]\n",
    "\n",
    "\n",
    "test_result_df = predict_cv_tabular_v1(\n",
    "    df=test_feature_df,\n",
    "    estimators=trained_estimators,\n",
    "    feature_columns=feature_columns,\n",
    "    test=True,\n",
    ")\n",
    "\n",
    "submission_df = make_submission(test_result_df)\n",
    "submission_filepath = Path(CFG.paths.output_dir) / f\"submissions_{CFG.experiment_name}.csv\"\n",
    "submission_df.to_csv(submission_filepath, index=False, header=False)\n",
    "\n",
    "submission_df_ = pd.read_csv(submission_filepath)\n",
    "\n",
    "assert len(submission_df_) == len(sample_submission_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
